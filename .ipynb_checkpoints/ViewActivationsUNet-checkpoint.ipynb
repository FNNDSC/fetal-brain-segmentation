{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from models.unet.unet import *\n",
    "from models.unet.unet_attention import *\n",
    "\n",
    "from datahandler import DataHandler\n",
    "import glob\n",
    "\n",
    "from keras import models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "plt.gray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGrid(act_i, images_per_row = 8, n_cols = 1):\n",
    "    \n",
    "    layer_activation = activations[act_i]\n",
    "    n_features = layer_activation.shape[-1]# Number of features in the feature map\n",
    "    \n",
    "    size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n",
    "    #n_cols = n_features // images_per_row # Tiles the activation channels in this matrix\n",
    "    display_grid = np.zeros((size * n_cols, images_per_row * size))\n",
    "\n",
    "    for col in range(n_cols): # Tiles each filter into a big horizontal grid\n",
    "        for row in range(images_per_row):\n",
    "            r = random.randint(0, n_features - 1)\n",
    "            channel_image = layer_activation[0,:, :,r]\n",
    "            channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n",
    "\n",
    "    return display_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSubplot(grid, name, g_shape, pos, rspan, cspan, no_cmap=False):\n",
    "    ax = plt.subplot2grid(grid_shape, pos, rowspan=rspan, colspan=cspan)\n",
    "    ax.set_title(name)\n",
    "    if no_cmap:\n",
    "        ax.imshow(grid, aspect='auto')\n",
    "    else:\n",
    "        ax.imshow(grid, aspect='auto', cmap='viridis')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0814 09:52:15.875723 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/AnacondaProjects/fetal-brain-segmentation/models/unet/unet.py:16: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "W0814 09:52:15.878762 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/AnacondaProjects/fetal-brain-segmentation/models/unet/unet.py:17: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "W0814 09:52:15.915311 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:98: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0814 09:52:15.925170 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:102: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0814 09:52:15.926117 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0814 09:52:15.964321 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0814 09:52:16.079336 4432442816 deprecation.py:506] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0814 09:52:16.147997 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
      "\n",
      "W0814 09:52:16.431856 4432442816 deprecation_wrapper.py:119] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0814 09:52:16.440737 4432442816 deprecation.py:323] From /Users/alejandrovaldes/anaconda3/envs/thesis/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "classifier = getUnet()\n",
    "layer_outputs = [layer.output for layer in classifier.layers[1:]] # Extracts the outputs of all\n",
    "activation_model = models.Model(inputs=classifier.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
    "activation_model.load_weights('./logs/unet/kfold_unet/kfold_unet_dice_DA_K9/kfold_unet_dice_DA_K9_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImageLoadingError",
     "evalue": "The supplied image data/prev/imgs/fetus_05.nii does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImageLoadingError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-64f7fc34dde0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimg_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/prev/imgs/fetus_05.nii'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetImageData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mimg_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimg_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AnacondaProjects/fetal-brain-segmentation/datahandler.py\u001b[0m in \u001b[0;36mgetImageData\u001b[0;34m(self, fname, is_mask)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;31m# get image data and header, must use med.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# for internal process of getting header info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;31m# switch axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/thesis/lib/python3.7/site-packages/medpy/io/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;31m# Check image file existence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImageLoadingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The supplied image {} does not exist.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# Try normal loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImageLoadingError\u001b[0m: The supplied image data/prev/imgs/fetus_05.nii does not exist."
     ]
    }
   ],
   "source": [
    "img_file = 'data/prev/Normal01/fetus_05.nii'\n",
    "dh = DataHandler()\n",
    "img, _ = dh.getImageData(img_file)\n",
    "img_slice = img[33]\n",
    "img_tensor = np.expand_dims(img_slice, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_layers = ['conv2d_2', 'conv2d_4', 'conv2d_6', 'conv2d_13', 'conv2d_8', 'conv2d_19', 'conv2d_16', 'conv2d_22']\n",
    "indexes = [1, 4, 7, 10, 20, 25, 30, 35]\n",
    "names = ['down block 1', 'down block 2', 'down block 3', 'down block 4', 'up block 1', 'up block 2', 'up block 3', 'up block 4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor) \n",
    "\n",
    "fig = plt.figure(1)\n",
    "cols = 20\n",
    "rows = 4\n",
    "grid_shape = (rows, cols)\n",
    "gridspec.GridSpec(rows, cols) \n",
    "\n",
    "i_up = -1\n",
    "\n",
    "for i in range(4):\n",
    "    n_rows = (i + 1) * 2\n",
    "    n_imgs = (i + 1) * 4\n",
    "    rspan = 1\n",
    "    cspan = 7\n",
    "    \n",
    "    grid = getGrid(indexes[i], n_imgs, n_rows)\n",
    "    plotSubplot(grid, names[i], grid_shape, (i,0), rspan=rspan, cspan=cspan)\n",
    "    grid = getGrid(indexes[-(i +1)], n_imgs, n_rows)\n",
    "    plotSubplot(grid, names[-(i + 1)], grid_shape, (i,cspan), rspan=rspan, cspan=cspan)\n",
    "\n",
    "grid = np.squeeze(img_slice)\n",
    "plotSubplot(grid, 'Input Imate', grid_shape, (0,14), rspan=2, cspan=8, no_cmap=True)\n",
    "\n",
    "grid = np.squeeze(activations[-1][0,:,:,0])\n",
    "plotSubplot(grid, 'Output', grid_shape, (2,14), rspan=2, cspan=8, no_cmap=True)\n",
    "\n",
    "fig.set_size_inches(w=14,h=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = getAttentionUnet()\n",
    "layer_outputs = [layer.output for layer in classifier.layers[1:]] # Extracts the outputs of all\n",
    "activation_model = models.Model(inputs=classifier.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
    "activation_model.load_weights('./logs/unet_attention/kfold_unet_attention/kfold_unet_attention_dice_DA_K9/kfold_unet_attention_dice_DA_K9_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted_layers = ['conv2d_2', 'conv2d_4', 'conv2d_6', 'conv2d_13', 'conv2d_8', 'conv2d_19', 'conv2d_16', 'conv2d_22']\n",
    "indexes = [1, 4, 7, 10, 26, 37, 48, 59]\n",
    "names = ['down block 1', 'down block 2', 'down block 3', 'down block 4', 'up block 1', 'up block 2', \n",
    "         'up block 3', 'up block 4']\n",
    "gates = [23, 34, 45, 56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor) \n",
    "print(len(activations))\n",
    "\n",
    "fig = plt.figure(1)\n",
    "cols = 29\n",
    "rows = 4\n",
    "grid_shape = (rows, cols)\n",
    "gridspec.GridSpec(rows, cols) \n",
    "\n",
    "i_up = -1\n",
    "\n",
    "for i in range(4):\n",
    "    n_rows = (i + 1) * 2\n",
    "    n_imgs = (i + 1) * 4\n",
    "    rspan = 1\n",
    "    cspan = 7\n",
    "    \n",
    "    grid = getGrid(indexes[i], n_imgs, n_rows)\n",
    "    plotSubplot(grid, names[i], grid_shape, (i,0), rspan=rspan, cspan=cspan)\n",
    "    \n",
    "    grid = getGrid(gates[-(i +1)], n_imgs, n_rows)\n",
    "    plotSubplot(grid, \"grid attention gate \" + str(i+1), grid_shape, (i, cspan), rspan=rspan, cspan=cspan)\n",
    "    \n",
    "    grid = getGrid(indexes[-(i +1)], n_imgs, n_rows)\n",
    "    plotSubplot(grid, names[-(i + 1)], grid_shape, (i, cspan * 2), rspan=rspan, cspan=cspan)\n",
    "    \n",
    "\n",
    "grid = (np.squeeze(img_slice) * 0.75) + (np.squeeze(activations[-1][0,:,:,0] * 255))\n",
    "plotSubplot(grid, 'Input Image + output', grid_shape, (0,21), rspan=2, cspan=8, no_cmap=True)\n",
    "\n",
    "grid = getGrid(gates[2], 2, 2)\n",
    "plotSubplot(grid, 'Zoom on grid attention gate 2', grid_shape, (2,21), rspan=2, cspan=8)\n",
    "\n",
    "fig.set_size_inches(w=16,h=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = getAttentionUnet()\n",
    "layer_outputs = [layer.output for layer in classifier.layers[1:]] # Extracts the outputs of all\n",
    "activation_model = models.Model(inputs=classifier.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input\n",
    "activation_model.load_weights('./logs/unet_attention/kfold_unet_attention/kfold_unet_attention_dice_DA_K9/kfold_unet_attention_dice_DA_K9_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
